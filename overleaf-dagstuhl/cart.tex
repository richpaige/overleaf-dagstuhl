Classification algorithms are a form of supervised machine learning for approximating functions mapping input features (e.g., our feature signature [3,3,1,0,1]) to a discrete output class (e.g., Tiger) from a finite set of possible values (e.g., [Tiger, Lion, Fan, Doctor, Zoo]. They require a training dataset with labelled examples of the output class to process, after which they can generalise from the previous examples to new unseen instances. For example, provided sufficient examples (i.e. diagram elements) of the form [3,3,1,0,1,Tiger] a classification algorithm can learn to predict the class Tiger when given an unlabelled example such as [3,2,1,0,1].

Many classification algorithms exist, some of the most established being decision trees, random forests, support vector machines and neural networks \cite{jiawei2001data}. For our previous work~\cite{zolotas2015inference} we chose to use decision trees due to the interpretable output representing the hypothesis learnt. In practice other classification algorithms can often have higher accuracy, but will produce a hypothesis in a form that is not human readable. As part of the extensions presented in this paper, we experiment also with Random Forests (RF)~\cite{Breiman2001}, a method that typically gives higher accuracy but less interpretable results~\cite{friedman2001elements}.

Specifically, for decision trees, we used the \textit{rpart} package (version 4.1-9)\footnote{http://cran.r-project.org/web/packages/rpart/index.html} that implements the functionality of CART \cite{breiman1984classification} in R\footnote{http://www.r-project.org/}. An example decision tree is illustrated in Fig.~\ref{fig:dtree}. Internal nodes represent conditions based on features (e.g., number of attributes, unique children, etc.), branches are labelled with ``TRUE'' or ``FALSE'' values for the condition of the parent node and leaf nodes represent the final classification given. To classify a new instance, the algorithm starts at the root of the tree and takes the branch that satisfies the condition of this node. The algorithm continues to process each internal node reached in the same manner until a leaf node is reached where the predicted classification of the new instance is the value of that leaf node. For example, given the tree in Fig.~\ref{fig:dtree}, a new instance with fewer than 1.5 unique incoming references (F2) and less than 3.5 attributes (F1) is classified as ``Fan'' (path is highlighted in Fig.~\ref{fig:dtree}).

\begin{figure}[h]
\centering
\includegraphics[width=10cm]{diagrams/cartExample.pdf}
\caption{Example Decision Tree}
\label{fig:dtree}
\end{figure}

CART generates a decision tree by considering all labelled instances in the training dataset in one single batch. For each input feature the information gain of using that feature to classify the instances in the batch is calculated. The feature with the highest information gain is used as the root node. The dataset is then split based on the values of the feature at the root node, and the process repeated on each child node with each subset of the dataset until a stop condition (e.g., minimum number of instances in a leaf node, depth or accuracy of tree) is satisfied.  

Additionally in this work, we used the \textit{randomForest} R package {(version 4.6-12)}\footnote{\url{https://cran.r-project.org/web/packages/randomForest/}} to compare the performance of CART against a method that typically provides higher accuracy. An RF is an ensemble of multiple decision trees, each trained on a different set of training instances from the dataset chosen at random with replacement and often using a random subset of the input features. Once trained, the ensemble classifies new instances by processing each tree in the same manner as an individual decision tree and then choosing a single predicted class by majority vote. Intuitively, this typically increases the accuracy in a manner similar to the wisdom of crowds. More formally, the combined multiple weak hypotheses in an RF will typically outperform the single hypothesis generated by CART due to each tree containing bias towards the data it observed but the ensemble being able to average out these biases. This advantage, however, is balanced by an increase in the complexity of the resultant model. Whilst it is simple to read a single decision tree and gain an understanding as to which features the model has correlated with a particular class, an ensemble of multiple trees becomes harder to read as many trees must be considered and the classifications of each combined to reach the final prediction of the model.

In our approach, the feature signatures list that contains the signatures of the known elements of the model are the input features to the CART and RF algorithms. A trained decision tree or ensemble of trees is produced dependent on the algorithm used. These can then be used to classify (identify the type of) the untyped nodes using their feature signatures. To compare the relative performance of CART and RF, the success of a classification algorithm can be evaluated by the accuracy of the resultant model (e.g., the decision tree learnt by CART) on test data not used when training. The accuracy of a model is the sum of true positives and negatives (i.e. all correctly classified instances) divided by the total number of instances in the test set. A single measure of accuracy can be artificially inflated due to the learnt model overfitting bias in the dataset used for training. To overcome this k-fold classification can be implemented \cite{mitchell1997machine}. This approach repeats the process of training the model and testing the accuracy $k$ times each time with a different split of the data into training and test data sets. The final accuracy using this method is then the mean value generated from the $k$ repeats. 

